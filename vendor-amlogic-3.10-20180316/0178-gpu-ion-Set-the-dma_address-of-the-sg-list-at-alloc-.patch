From 891671dde6b0723a52bd8e6e45fe3aff968fb8fc Mon Sep 17 00:00:00 2001
From: Rebecca Schultz Zavin <rebecca@android.com>
Date: Thu, 7 Jun 2012 14:51:21 -0700
Subject: [PATCH 0178/5965] gpu: ion: Set the dma_address of the sg list at
 alloc time

This patch sets the dma_address field of the sglist representing
an allocation at allocation time.  This technically breaks the dma api
which states that these addresses should be set when a particular device
takes ownership of a buffer via the dma_map apis.  In the case of our
systems the only dma address space is physical addresses.  Additionally,
we can not afford the overhead of calling dma_map_sg from this location
as it implies a cache invalidate that is not necessary if the memory
was previously mapped cached.  Instead, the expectation is that memory
being returned from the heaps is ready for dma in that if any cached
mappings of that memory exist they have been invalidated.

Change-Id: Ib3025e80cee52ac0e9d809291e0196a084ea97bd
Signed-off-by: Rebecca Schultz Zavin <rebecca@android.com>
---
 drivers/gpu/ion/ion.c | 13 ++++++++++++-
 1 file changed, 12 insertions(+), 1 deletion(-)

diff --git a/drivers/gpu/ion/ion.c b/drivers/gpu/ion/ion.c
index 65582fb0a019..36b723c201e9 100644
--- a/drivers/gpu/ion/ion.c
+++ b/drivers/gpu/ion/ion.c
@@ -136,7 +136,8 @@ static struct ion_buffer *ion_buffer_create(struct ion_heap *heap,
 {
 	struct ion_buffer *buffer;
 	struct sg_table *table;
-	int ret;
+	struct scatterlist *sg;
+	int i, ret;
 
 	buffer = kzalloc(sizeof(struct ion_buffer), GFP_KERNEL);
 	if (!buffer)
@@ -163,6 +164,16 @@ static struct ion_buffer *ion_buffer_create(struct ion_heap *heap,
 	buffer->sg_table = table;
 
 	mutex_init(&buffer->lock);
+	/* this will set up dma addresses for the sglist -- it is not
+	   technically correct as per the dma api -- a specific
+	   device isn't really taking ownership here.  However, in practice on
+	   our systems the only dma_address space is physical addresses.
+	   Additionally, we can't afford the overhead of invalidating every
+	   allocation via dma_map_sg. The implicit contract here is that
+	   memory comming from the heaps is ready for dma, ie if it has a
+	   cached mapping that mapping has been invalidated */
+	for_each_sg(buffer->sg_table->sgl, sg, buffer->sg_table->nents, i)
+		sg_dma_address(sg) = sg_phys(sg);
 	ion_buffer_add(dev, buffer);
 	return buffer;
 }
-- 
2.19.0

