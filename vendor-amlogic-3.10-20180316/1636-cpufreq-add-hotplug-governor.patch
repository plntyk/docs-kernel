From 4b8b5386aa3cd0d7547326ee58b1c7e353a5e9c4 Mon Sep 17 00:00:00 2001
From: "shi.liu" <shi.liu@amlogic.com>
Date: Mon, 4 Nov 2013 12:30:47 +0800
Subject: [PATCH 1636/5965] cpufreq:add hotplug governor

---
 drivers/cpufreq/cpufreq_hotplug.c | 798 ++++++++++++++++++++++++++++++
 1 file changed, 798 insertions(+)
 create mode 100755 drivers/cpufreq/cpufreq_hotplug.c

diff --git a/drivers/cpufreq/cpufreq_hotplug.c b/drivers/cpufreq/cpufreq_hotplug.c
new file mode 100755
index 000000000000..22d2c914cde8
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_hotplug.c
@@ -0,0 +1,798 @@
+/*
+ * CPUFreq hotplug governor
+ *
+ * Copyright (C) 2010 Texas Instruments, Inc.
+ *   Mike Turquette <mturquette@ti.com>
+ *   Santosh Shilimkar <santosh.shilimkar@ti.com>
+ *
+ * Based on ondemand governor
+ * Copyright (C)  2001 Russell King
+ *           (C)  2003 Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>,
+ *                     Jun Nakajima <jun.nakajima@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/cpufreq.h>
+#include <linux/cpu.h>
+#include <linux/jiffies.h>
+#include <linux/kernel_stat.h>
+#include <linux/mutex.h>
+#include <linux/hrtimer.h>
+#include <linux/tick.h>
+#include <linux/ktime.h>
+#include <linux/sched.h>
+#include <linux/err.h>
+#include <linux/slab.h>
+#include <linux/kthread.h>
+#include <plat/io.h>
+#include <mach/io.h>
+#include <mach/register.h>
+#include "cpufreq_governor.h"
+
+/* greater than 80% avg load across online CPUs increases frequency */
+#define DEFAULT_UP_FREQ_MIN_LOAD			(80)
+
+/* Keep 10% of idle under the up threshold when decreasing the frequency */
+#define DEFAULT_FREQ_DOWN_DIFFERENTIAL			(20)
+
+/* less than 35% avg load across online CPUs decreases frequency */
+#define DEFAULT_DOWN_FREQ_MAX_LOAD			(35)
+
+/* default sampling period (uSec) is bogus; 10x ondemand's default for x86 */
+#define DEFAULT_SAMPLING_PERIOD				(100000)
+
+/* default number of sampling periods to average before hotplug-in decision */
+#define DEFAULT_HOTPLUG_IN_SAMPLING_PERIODS		(5)
+
+/* default number of sampling periods to average before hotplug-out decision */
+#define DEFAULT_HOTPLUG_OUT_SAMPLING_PERIODS		(20)
+#define CPU_HOTPLUG_NONE 0
+#define CPU_HOTPLUG_PLUG 1
+#define CPU_HOTPLUG_UNPLUG 2
+extern u64 get_cpu_idle_time(unsigned int cpu, u64 *wall);
+extern int select_cpu_for_hotplug(struct task_struct *p, int sd_flags, int wake_flags);
+
+static DEFINE_PER_CPU(struct hg_cpu_dbs_info_s, hg_cpu_dbs_info);
+static unsigned int hispeed_freq = 816000;
+
+static cpumask_var_t new_mask;
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_HOTPLUG
+static struct cpufreq_governor cpufreq_gov_hotplug;
+#endif
+static struct task_struct *cpu_hotplug_task;
+static int cpu_hotplug_flag = 0;
+static struct hg_dbs_tuners hg_tuners = {
+	.up_threshold =			DEFAULT_UP_FREQ_MIN_LOAD,
+	.down_differential =            DEFAULT_FREQ_DOWN_DIFFERENTIAL,
+	.down_threshold =		DEFAULT_DOWN_FREQ_MAX_LOAD,
+	.hotplug_in_sampling_periods =	DEFAULT_HOTPLUG_IN_SAMPLING_PERIODS,
+	.hotplug_out_sampling_periods =	DEFAULT_HOTPLUG_OUT_SAMPLING_PERIODS,
+	.hotplug_load_index =		0,
+	.ignore_nice =			0,
+	.io_is_busy =			0,
+	.cpu_num_unplug_once = 2,
+	.cpu_num_plug_once = 1,
+	.hotplug_min_freq  = 96000,
+	.hotplug_max_freq  = 96000,
+};
+
+static DEFINE_PER_CPU(struct hg_cpu_dbs_info_s, hp_cpu_dbs_info);
+
+static unsigned int dbs_enable;	/* number of CPUs using this policy */
+
+static DEFINE_MUTEX(dbs_mutex);
+
+static struct task_struct *NULL_task = NULL;
+/************************** sysfs interface ************************/
+
+/* XXX look at global sysfs macros in cpufreq.h, can those be used here? */
+
+/* cpufreq_hotplug Governor Tunables */
+#define show_one(file_name, object)					\
+static ssize_t show_##file_name						\
+(struct kobject *kobj, struct attribute *attr, char *buf)		\
+{									\
+	return sprintf(buf, "%u\n", hg_tuners.object);		\
+}
+show_one(sampling_rate, sampling_rate);
+show_one(up_threshold, up_threshold);
+show_one(down_differential, down_differential);
+show_one(down_threshold, down_threshold);
+show_one(hotplug_in_sampling_periods, hotplug_in_sampling_periods);
+show_one(hotplug_out_sampling_periods, hotplug_out_sampling_periods);
+show_one(ignore_nice_load, ignore_nice);
+show_one(io_is_busy, io_is_busy);
+show_one(cpu_num_unplug_once, cpu_num_unplug_once);
+show_one(cpu_num_plug_once, cpu_num_plug_once);
+show_one(hotplug_min_freq, hotplug_min_freq);
+show_one(hotplug_max_freq, hotplug_max_freq);
+
+static ssize_t store_sampling_rate(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	mutex_lock(&dbs_mutex);
+	hg_tuners.sampling_rate = input;
+	mutex_unlock(&dbs_mutex);
+
+	return count;
+}
+
+static ssize_t store_up_threshold(struct kobject *a, struct attribute *b,
+				  const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input <= hg_tuners.down_threshold) {
+		return -EINVAL;
+	}
+
+	mutex_lock(&dbs_mutex);
+	hg_tuners.up_threshold = input;
+	mutex_unlock(&dbs_mutex);
+
+	return count;
+}
+
+static ssize_t store_down_differential(struct kobject *a, struct attribute *b,
+				  const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input >= hg_tuners.up_threshold)
+		return -EINVAL;
+
+	mutex_lock(&dbs_mutex);
+	hg_tuners.down_differential = input;
+	mutex_unlock(&dbs_mutex);
+
+	return count;
+}
+
+static ssize_t store_down_threshold(struct kobject *a, struct attribute *b,
+				  const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input >= hg_tuners.up_threshold) {
+		return -EINVAL;
+	}
+
+	mutex_lock(&dbs_mutex);
+	hg_tuners.down_threshold = input;
+	mutex_unlock(&dbs_mutex);
+
+	return count;
+}
+
+static ssize_t store_hotplug_in_sampling_periods(struct kobject *a,
+		struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	unsigned int *temp;
+	unsigned int max_windows;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1)
+		return -EINVAL;
+
+	/* already using this value, bail out */
+	if (input == hg_tuners.hotplug_in_sampling_periods)
+		return count;
+
+	mutex_lock(&dbs_mutex);
+	ret = count;
+	max_windows = max(hg_tuners.hotplug_in_sampling_periods,
+			hg_tuners.hotplug_out_sampling_periods);
+
+	/* no need to resize array */
+	if (input <= max_windows) {
+		hg_tuners.hotplug_in_sampling_periods = input;
+		goto out;
+	}
+
+	/* resize array */
+	temp = kmalloc((sizeof(unsigned int) * input), GFP_KERNEL);
+
+	if (!temp || IS_ERR(temp)) {
+		ret = -ENOMEM;
+		goto out;
+	}
+
+	memcpy(temp, hg_tuners.hotplug_load_history,
+			(max_windows * sizeof(unsigned int)));
+	kfree(hg_tuners.hotplug_load_history);
+
+	/* replace old buffer, old number of sampling periods & old index */
+	hg_tuners.hotplug_load_history = temp;
+	hg_tuners.hotplug_in_sampling_periods = input;
+	hg_tuners.hotplug_load_index = max_windows;
+out:
+	mutex_unlock(&dbs_mutex);
+
+	return ret;
+}
+
+static ssize_t store_hotplug_out_sampling_periods(struct kobject *a,
+		struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	unsigned int *temp;
+	unsigned int max_windows;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1)
+		return -EINVAL;
+
+	/* already using this value, bail out */
+	if (input == hg_tuners.hotplug_out_sampling_periods)
+		return count;
+
+	mutex_lock(&dbs_mutex);
+	ret = count;
+	max_windows = max(hg_tuners.hotplug_in_sampling_periods,
+			hg_tuners.hotplug_out_sampling_periods);
+
+	/* no need to resize array */
+	if (input <= max_windows) {
+		hg_tuners.hotplug_out_sampling_periods = input;
+		goto out;
+	}
+
+	/* resize array */
+	temp = kmalloc((sizeof(unsigned int) * input), GFP_KERNEL);
+
+	if (!temp || IS_ERR(temp)) {
+		ret = -ENOMEM;
+		goto out;
+	}
+
+	memcpy(temp, hg_tuners.hotplug_load_history,
+			(max_windows * sizeof(unsigned int)));
+	kfree(hg_tuners.hotplug_load_history);
+
+	/* replace old buffer, old number of sampling periods & old index */
+	hg_tuners.hotplug_load_history = temp;
+	hg_tuners.hotplug_out_sampling_periods = input;
+	hg_tuners.hotplug_load_index = max_windows;
+out:
+	mutex_unlock(&dbs_mutex);
+
+	return ret;
+}
+
+static ssize_t store_ignore_nice_load(struct kobject *a, struct attribute *b,
+				      const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	unsigned int j;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input > 1)
+		input = 1;
+
+	mutex_lock(&dbs_mutex);
+	if (input == hg_tuners.ignore_nice) { /* nothing to do */
+		mutex_unlock(&dbs_mutex);
+		return count;
+	}
+	hg_tuners.ignore_nice = input;
+
+	/* we need to re-evaluate prev_cpu_idle */
+	for_each_online_cpu(j) {
+		struct hg_cpu_dbs_info_s *dbs_info;
+		dbs_info = &per_cpu(hp_cpu_dbs_info, j);
+		dbs_info->cdbs.prev_cpu_idle = get_cpu_idle_time(j,
+						&dbs_info->cdbs.prev_cpu_wall);
+		if (hg_tuners.ignore_nice)
+			dbs_info->cdbs.prev_cpu_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+
+	}
+	mutex_unlock(&dbs_mutex);
+
+	return count;
+}
+
+static ssize_t store_io_is_busy(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	mutex_lock(&dbs_mutex);
+	hg_tuners.io_is_busy = !!input;
+	mutex_unlock(&dbs_mutex);
+
+	return count;
+}
+
+static ssize_t store_cpu_num_unplug_once(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	if(input >= NR_CPUS || input <= 0){
+		return -EINVAL;
+	}
+
+	mutex_lock(&dbs_mutex);
+	hg_tuners.cpu_num_unplug_once = input;
+	mutex_unlock(&dbs_mutex);
+
+	return count;
+}
+
+static ssize_t store_cpu_num_plug_once(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	if(input >= NR_CPUS || input <= 0){
+		return -EINVAL;
+	}
+
+	mutex_lock(&dbs_mutex);
+	hg_tuners.cpu_num_plug_once = input;
+	mutex_unlock(&dbs_mutex);
+
+	return count;
+}
+static ssize_t store_hotplug_min_freq(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	if(input >= NR_CPUS || input <= 0){
+		return -EINVAL;
+	}
+
+	mutex_lock(&dbs_mutex);
+	hg_tuners.hotplug_min_freq = input;
+	mutex_unlock(&dbs_mutex);
+
+	return count;
+}
+
+static ssize_t store_hotplug_max_freq(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	if(input >= NR_CPUS || input <= 0){
+		return -EINVAL;
+	}
+
+	mutex_lock(&dbs_mutex);
+	hg_tuners.hotplug_max_freq = input;
+	mutex_unlock(&dbs_mutex);
+
+	return count;
+}
+define_one_global_rw(sampling_rate);
+define_one_global_rw(up_threshold);
+define_one_global_rw(down_differential);
+define_one_global_rw(down_threshold);
+define_one_global_rw(hotplug_in_sampling_periods);
+define_one_global_rw(hotplug_out_sampling_periods);
+define_one_global_rw(ignore_nice_load);
+define_one_global_rw(io_is_busy);
+define_one_global_rw(cpu_num_unplug_once);
+define_one_global_rw(cpu_num_plug_once);
+define_one_global_rw(hotplug_min_freq);
+define_one_global_rw(hotplug_max_freq);
+
+static struct attribute *dbs_attributes[] = {
+	&sampling_rate.attr,
+	&up_threshold.attr,
+	&down_differential.attr,
+	&down_threshold.attr,
+	&hotplug_in_sampling_periods.attr,
+	&hotplug_out_sampling_periods.attr,
+	&ignore_nice_load.attr,
+	&io_is_busy.attr,
+	&cpu_num_unplug_once.attr,
+	&cpu_num_plug_once.attr,
+	&hotplug_min_freq.attr,
+	&hotplug_max_freq.attr,
+	NULL
+};
+
+static struct attribute_group hg_attr_group = {
+	.attrs = dbs_attributes,
+	.name = "hotplug",
+};
+
+/************************** sysfs end ************************/
+static int hg_init(struct dbs_data *dbs_data)
+{
+	struct hg_dbs_tuners *tuners;
+
+	tuners = kzalloc(sizeof(struct hg_dbs_tuners), GFP_KERNEL);
+	if (!tuners) {
+		pr_err("%s: kzalloc failed\n", __func__);
+		return -ENOMEM;
+	}
+
+	tuners->down_differential			=	DEFAULT_FREQ_DOWN_DIFFERENTIAL;
+	tuners->down_threshold				=	DEFAULT_DOWN_FREQ_MAX_LOAD;
+	tuners->hotplug_in_sampling_periods =	DEFAULT_HOTPLUG_IN_SAMPLING_PERIOD;
+	tuners->hotplug_out_sampling_periods =	DEFAULT_HOTPLUG_OUT_SAMPLING_PERIODS;
+	htuners->otplug_load_index			=	0;
+	tuners->ignore_nice_load			=	0;
+	tuners->io_is_busy					=	0;
+	tuners->cpu_num_unplug_once			=	2;
+	tuners->cpu_num_plug_once			=	1;
+	tuners->hotplug_min_freq			=	96000;
+	tuners->hotplug_max_freq			=	96000;
+
+	dbs_data->tuners = tuners;
+	dbs_data->min_sampling_rate = MIN_SAMPLING_RATE_RATIO *
+		jiffies_to_usecs(10);
+	mutex_init(&dbs_data->mutex);
+	return 0;
+}
+
+static void hg_exit(struct dbs_data *dbs_data)
+{
+	kfree(dbs_data->tuners);
+}
+static void cpu_hotplug_thread(int *hotplug_flag)
+{
+	int i, j,target_cpu = 1;
+	unsigned long flags, cpu_down_num;
+
+	while(1){
+		if (kthread_should_stop())
+			break;
+
+		if(*hotplug_flag == CPU_HOTPLUG_PLUG){
+			*hotplug_flag = CPU_HOTPLUG_NONE;
+			j = 0;
+			for(i = 0; i < NR_CPUS; i++){
+				if(cpu_online(i))
+					continue;
+				j++;
+				printk("1---cpu up\n");
+				cpu_up(i);
+				printk("2---cpu up\n");
+				cpumask_set_cpu(i, tsk_cpus_allowed(NULL_task));
+				if(j >= hg_tuners.cpu_num_plug_once)
+					break;
+			}
+		}else if(*hotplug_flag == CPU_HOTPLUG_UNPLUG){
+			*hotplug_flag = CPU_HOTPLUG_NONE;
+			cpu_down_num = 0;
+			for(i = 0; i < num_online_cpus()-1; i++){
+				raw_spin_lock_irqsave(&NULL_task->pi_lock, flags);
+				target_cpu = select_cpu_for_hotplug(NULL_task, SD_BALANCE_EXEC, 0);
+				raw_spin_unlock_irqrestore(&NULL_task->pi_lock, flags);
+				if(target_cpu == 0){
+					i--;
+					goto clear_cpu;
+				}
+				if(!cpu_active(target_cpu)){
+					goto clear_cpu;
+				}
+				printk("1---cpu down\n");
+				cpu_down(target_cpu);
+				printk("2---cpu down\n");
+				cpu_down_num++;
+clear_cpu:
+				cpumask_clear_cpu(target_cpu, tsk_cpus_allowed(NULL_task));
+				if(cpu_down_num >= hg_tuners.cpu_num_unplug_once){
+					break;
+				}
+			}
+		}
+
+		set_current_state(TASK_INTERRUPTIBLE);
+		schedule();
+		set_current_state(TASK_RUNNING);
+	}
+}
+static void hg_check_cpu(int cpu, unsigned int max_load)
+{
+	/* largest CPU load in terms of frequency */
+	unsigned int max_load_freq = 0;
+	/* average load across all enabled CPUs */
+	unsigned int avg_load = 0;
+	/* average load across multiple sampling periods for hotplug events */
+	unsigned int hotplug_in_avg_load = 0;
+	unsigned int hotplug_out_avg_load = 0;
+	/* number of sampling periods averaged for hotplug decisions */
+	unsigned int periods;
+	unsigned int i, j;
+	int ret = 0;
+
+	struct hg_cpu_dbs_info_s *dbs_info = &per_cpu(hg_cpu_dbs_info, cpu);
+	struct cpufreq_policy *policy = dbs_info->cdbs.cur_policy;
+
+	avg_load = hg_tuners.hotplug_load_history[hg_tuners.hotplug_load_index];
+	max_load_freq = hg_tuners.max_load_freq;
+	/*
+	 * hotplug load accounting
+	 * average load over multiple sampling periods
+	 */
+	/* how many sampling periods do we use for hotplug decisions? */
+	periods = max(hg_tuners.hotplug_in_sampling_periods,
+			hg_tuners.hotplug_out_sampling_periods);
+
+	/* compute average load across in & out sampling periods */
+	for (i = 0, j = hg_tuners.hotplug_load_index;
+			i < periods; i++, j--) {
+		if (i < hg_tuners.hotplug_in_sampling_periods)
+			hotplug_in_avg_load +=
+				hg_tuners.hotplug_load_history[j];
+		if (i < hg_tuners.hotplug_out_sampling_periods)
+			hotplug_out_avg_load +=
+				hg_tuners.hotplug_load_history[j];
+
+		if (j == 0)
+			j = periods;
+	}
+
+	hotplug_in_avg_load = hotplug_in_avg_load /
+		hg_tuners.hotplug_in_sampling_periods;
+
+	hotplug_out_avg_load = hotplug_out_avg_load /
+		hg_tuners.hotplug_out_sampling_periods;
+
+	/* return to first element if we're at the circular buffer's end */
+	if (++hg_tuners.hotplug_load_index == periods)
+		hg_tuners.hotplug_load_index = 0;
+
+	/* check if auxiliary CPU is needed based on avg_load */
+	if (avg_load > hg_tuners.up_threshold) {
+		/* should we enable auxillary CPUs? */
+		if (num_online_cpus() < NR_CPUS && hotplug_in_avg_load >
+			hg_tuners.up_threshold && policy->cur >=  hg_tuners.hotplug_max_freq) {
+			/* hotplug with cpufreq is nasty
+			 * a call to cpufreq_governor_dbs may cause a lockup.
+			 * wq is not running here so its safe.
+			 */
+			cpu_hotplug_flag = CPU_HOTPLUG_PLUG;
+			wake_up_process(cpu_hotplug_task);
+			goto out;
+		}
+	}
+
+	/* check for frequency increase based on max_load */
+	if (max_load > hg_tuners.up_threshold) {
+		/* increase to highest frequency supported */
+		if (policy->cur < policy->max)
+			__cpufreq_driver_target(policy, policy->max,
+					CPUFREQ_RELATION_H);
+		goto out;
+	}
+
+	/* check for frequency decrease */
+	if (avg_load < hg_tuners.down_threshold) {
+		/* are we at the minimum frequency already? */
+		if (policy->cur <= hg_tuners.hotplug_min_freq) {
+			/* should we disable auxillary CPUs? */
+			if (num_online_cpus() > 1 && hotplug_out_avg_load <
+				hg_tuners.down_threshold) {
+				cpu_hotplug_flag = CPU_HOTPLUG_UNPLUG;
+				wake_up_process(cpu_hotplug_task);
+				goto out;
+			}
+		}
+	}
+
+	if ((max_load_freq >
+	    (hg_tuners.up_threshold - hg_tuners.down_differential) *
+	     policy->cur) && (policy->cur < policy->max)) {
+		unsigned int freq_next;
+
+		freq_next = hispeed_freq;
+
+		if (freq_next < policy->min)
+			freq_next = policy->min;
+		if (freq_next > policy->max)
+			freq_next = policy->max;
+
+		if(freq_next == policy->cur)
+			goto out;
+
+		 __cpufreq_driver_target(policy, freq_next,
+					 CPUFREQ_RELATION_L);
+		goto out;
+	}
+	/*
+	 * go down to the lowest frequency which can sustain the load by
+	 * keeping 30% of idle in order to not cross the up_threshold
+	 */
+	if ((max_load_freq <
+	    (hg_tuners.up_threshold - hg_tuners.down_differential) *
+	     policy->cur) && (policy->cur > policy->min)) {
+		unsigned int freq_next;
+
+		freq_next = max_load_freq /
+				(hg_tuners.up_threshold -
+				 hg_tuners.down_differential);
+
+		if (freq_next < policy->min)
+			freq_next = policy->min;
+
+		if(freq_next == policy->cur)
+			goto out;
+
+		 __cpufreq_driver_target(policy, freq_next,
+					 CPUFREQ_RELATION_L);
+	}
+out:
+	//printk("hg dbs: %u %u %u o avg:%u i avg:%u %u \n", policy->cur, policy->min,
+	//	   max_load_freq, hotplug_out_avg_load, hotplug_in_avg_load, max_load);
+	return;
+}
+
+static void hg_dbs_timer(struct work_struct *work)
+{
+	struct hg_cpu_dbs_info_s *dbs_info =
+		container_of(work, struct hg_cpu_dbs_info_s, cdbs.work.work);
+	unsigned int cpu = dbs_info->cdbs.cpu;
+	unsigned long flags;
+
+	/* We want all related CPUs to do sampling nearly on same jiffy */
+	int delay = delay_for_sampling_rate(hg_tuners.sampling_rate);
+	mutex_lock(&dbs_info->cdbs.timer_mutex);
+	dbs_check_cpu(&hg_dbs_data, cpu);
+	schedule_delayed_work_on(cpu, &dbs_info->cdbs.work, delay);
+	mutex_unlock(&dbs_info->cdbs.timer_mutex);
+}
+
+define_get_cpu_dbs_routines(hg_cpu_dbs_info);
+
+static struct hg_ops hg_ops = {
+};
+
+static struct common_dbs_data hg_dbs_data = {
+	.governor = GOV_HOTPLUG,
+	.attr_group = &hg_attr_group,
+	.tuners = &hg_tuners,
+	.get_cpu_cdbs = get_cpu_cdbs,
+	.get_cpu_dbs_info_s = get_cpu_dbs_info_s,
+	.gov_dbs_timer = hg_dbs_timer,
+	.gov_check_cpu = hg_check_cpu,
+	.gov_ops = &hg_ops,
+	.init = hg_init,
+	.exit = hg_exit,
+};
+
+static int hg_cpufreq_governor_dbs(struct cpufreq_policy *policy,
+		unsigned int event)
+{
+	return cpufreq_governor_dbs(&hg_dbs_data, policy, event);
+}
+static void do_null_task()
+{
+	printk("---add for hotplug governor\n");
+}
+static struct cpufreq_governor cpufreq_gov_hotplug = {
+	.name			= "hotplug",
+	.governor		= hg_cpufreq_governor_dbs,
+	.max_transition_latency	= TRANSITION_LATENCY_LIMIT,
+	.owner			= THIS_MODULE,
+};
+static int __init cpufreq_gov_dbs_init(void)
+{
+	int err, i = 0;
+	cputime64_t wall;
+	u64 idle_time;
+	int cpu = get_cpu();
+	struct sched_param param = { .sched_priority = MAX_RT_PRIO-1 };
+
+	mutex_init(&hg_dbs_data.mutex);
+	idle_time = get_cpu_idle_time_us(cpu, &wall);
+	put_cpu();
+
+	if (idle_time != -1ULL) {
+		hg_tuners.up_threshold = DEFAULT_UP_FREQ_MIN_LOAD;
+	} else {
+		pr_err("cpufreq-hotplug: %s: assumes CONFIG_NO_HZ\n",
+				__func__);
+		return -EINVAL;
+	}
+
+	if (!alloc_cpumask_var(&new_mask, GFP_KERNEL))
+		return -ENOMEM;
+	/*******************NULL task******************/
+	NULL_task = kthread_create(do_null_task, NULL, "NULL_task_for_hotplug");
+	if(!NULL_task){
+		err = PTR_ERR(NULL_task);
+		NULL_task = NULL;
+		return err;
+	}
+
+	for(i = 1; i < NR_CPUS; i++){
+		cpumask_set_cpu(i, tsk_cpus_allowed(NULL_task));
+	}
+	cpumask_clear_cpu(0, tsk_cpus_allowed(NULL_task));
+
+
+	/*******************cpu hotplug task******************/
+	cpu_hotplug_task =
+		kthread_create(cpu_hotplug_thread, &cpu_hotplug_flag,
+			       "cpu_hotplug_gdbs");
+	if (IS_ERR(cpu_hotplug_task))
+		return PTR_ERR(cpu_hotplug_task);
+
+	sched_setscheduler_nocheck(cpu_hotplug_task, SCHED_FIFO, &param);
+	get_task_struct(cpu_hotplug_task);
+	cpu_hotplug_flag = CPU_HOTPLUG_NONE;
+	/* wake up so the thread does not look hung to the freezer */
+	wake_up_process(cpu_hotplug_task);
+
+	err = cpufreq_register_governor(&cpufreq_gov_hotplug);
+
+	return err;
+}
+
+static void __exit cpufreq_gov_dbs_exit(void)
+{
+	cpufreq_unregister_governor(&cpufreq_gov_hotplug);
+	free_cpumask_var(new_mask);
+	kthread_stop(NULL_task);
+	kthread_stop(cpu_hotplug_task);
+	put_task_struct(cpu_hotplug_task);
+}
+
+MODULE_AUTHOR("Mike Turquette <mturquette@ti.com>");
+MODULE_DESCRIPTION("'cpufreq_hotplug' - cpufreq governor for dynamic frequency scaling and CPU hotplugging");
+MODULE_LICENSE("GPL");
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_HOTPLUG
+fs_initcall(cpufreq_gov_dbs_init);
+#else
+module_init(cpufreq_gov_dbs_init);
+#endif
+module_exit(cpufreq_gov_dbs_exit);
-- 
2.19.0

