From fa132ca5eb61942973fe87b8214640778f6ec453 Mon Sep 17 00:00:00 2001
From: "shi.liu" <shi.liu@amlogic.com>
Date: Wed, 27 Nov 2013 15:20:08 +0800
Subject: [PATCH 2076/5965] cpufreq:optimize hotplug governor for interactive
 processing

---
 drivers/cpufreq/cpufreq_governor.c |  25 +++++-
 drivers/cpufreq/cpufreq_governor.h |   7 +-
 drivers/cpufreq/cpufreq_hotplug.c  | 126 +++++++++++++++++++++++++++--
 3 files changed, 147 insertions(+), 11 deletions(-)

diff --git a/drivers/cpufreq/cpufreq_governor.c b/drivers/cpufreq/cpufreq_governor.c
index 7c653f7d8187..dfe43113f742 100755
--- a/drivers/cpufreq/cpufreq_governor.c
+++ b/drivers/cpufreq/cpufreq_governor.c
@@ -16,6 +16,7 @@
 
 #define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
 
+#include <linux/cpu.h>
 #include <asm/cputime.h>
 #include <linux/cpufreq.h>
 #include <linux/cpumask.h>
@@ -26,6 +27,7 @@
 #include <linux/tick.h>
 #include <linux/types.h>
 #include <linux/workqueue.h>
+#include <linux/notifier.h>
 
 #include "cpufreq_governor.h"
 
@@ -198,7 +200,7 @@ void gov_queue_work(struct dbs_data *dbs_data, struct cpufreq_policy *policy,
 }
 EXPORT_SYMBOL_GPL(gov_queue_work);
 
-static inline void gov_cancel_work(struct dbs_data *dbs_data,
+void gov_cancel_work(struct dbs_data *dbs_data,
 		struct cpufreq_policy *policy)
 {
 	struct cpu_dbs_common_info *cdbs;
@@ -255,6 +257,7 @@ int cpufreq_governor_dbs(struct cpufreq_policy *policy,
 	struct cs_cpu_dbs_info_s *cs_dbs_info = NULL;
 	struct hg_cpu_dbs_info_s *hg_dbs_info = NULL;
 	struct od_ops *od_ops = NULL;
+	struct hg_ops *hg_ops = NULL;
 	struct od_dbs_tuners *od_tuners = NULL;
 	struct cs_dbs_tuners *cs_tuners = NULL;
 	struct hg_dbs_tuners *hg_tuners = NULL;
@@ -411,6 +414,10 @@ int cpufreq_governor_dbs(struct cpufreq_policy *policy,
 			cs_dbs_info->enable = 1;
 			cs_dbs_info->requested_freq = policy->cur;
 		}else if(dbs_data->cdata->governor == GOV_HOTPLUG){
+			for_each_cpu(j, policy->cpus) {
+				hg_dbs_info = dbs_data->cdata->get_cpu_dbs_info_s(j);
+				hg_dbs_info->enable = 1;
+			}
 			hg_tuners = dbs_data->tuners;
 			max_periods = max(DEFAULT_HOTPLUG_IN_SAMPLING_PERIODS,
 					DEFAULT_HOTPLUG_OUT_SAMPLING_PERIODS);
@@ -423,13 +430,15 @@ int cpufreq_governor_dbs(struct cpufreq_policy *policy,
 			}
 			for (i = 0; i < max_periods; i++)
 				hg_tuners->hotplug_load_history[i] = 50;
+
+			hg_ops = dbs_data->cdata->gov_ops;
+			idle_notifier_register(hg_ops->notifier_block);
 		}
 		else if(dbs_data->cdata->governor == GOV_ONDEMAND) {
 			od_dbs_info->rate_mult = 1;
 			od_dbs_info->sample_type = OD_NORMAL_SAMPLE;
 			od_ops->powersave_bias_init_cpu(cpu);
 		}
-
 		mutex_unlock(&dbs_data->mutex);
 
 		/* Initiate timer time stamp */
@@ -447,13 +456,23 @@ int cpufreq_governor_dbs(struct cpufreq_policy *policy,
 	case CPUFREQ_GOV_STOP:
 		if (dbs_data->cdata->governor == GOV_CONSERVATIVE)
 			cs_dbs_info->enable = 0;
+		if(dbs_data->cdata->governor == GOV_HOTPLUG){
+			for_each_cpu(j, policy->cpus) {
+				hg_dbs_info = dbs_data->cdata->get_cpu_dbs_info_s(j);
+				hg_dbs_info->enable = 0;
+			}
+			mutex_lock(&dbs_data->mutex);
+			hg_ops = dbs_data->cdata->gov_ops;
+			idle_notifier_unregister(hg_ops->notifier_block);
+			mutex_unlock(&dbs_data->mutex);
+		}
 
 		gov_cancel_work(dbs_data, policy);
 
 		mutex_lock(&dbs_data->mutex);
 		mutex_destroy(&cpu_cdbs->timer_mutex);
-
 		mutex_unlock(&dbs_data->mutex);
+
 		if(dbs_data->cdata->governor == GOV_HOTPLUG){
 			hg_tuners = dbs_data->tuners;
 			if(hg_tuners->hotplug_load_history){
diff --git a/drivers/cpufreq/cpufreq_governor.h b/drivers/cpufreq/cpufreq_governor.h
index 5517e10a4dc1..6a223ef7c518 100755
--- a/drivers/cpufreq/cpufreq_governor.h
+++ b/drivers/cpufreq/cpufreq_governor.h
@@ -153,6 +153,8 @@ struct cpu_dbs_common_info {
 struct hg_cpu_dbs_info_s {
 	struct cpu_dbs_common_info cdbs;
 	struct cpufreq_frequency_table *freq_table;
+	unsigned int requested_freq;
+	unsigned int enable:1;
 };
 struct od_cpu_dbs_info_s {
 	struct cpu_dbs_common_info cdbs;
@@ -258,8 +260,7 @@ struct cs_ops {
 };
 
 struct hg_ops {
-	int (*io_busy)(void);
-	void (*powersave_bias_init_cpu)(int cpu);
+	struct notifier_block *notifier_block;
 };
 static inline int delay_for_sampling_rate(unsigned int sampling_rate)
 {
@@ -295,6 +296,8 @@ int cpufreq_governor_dbs(struct cpufreq_policy *policy,
 		struct common_dbs_data *cdata, unsigned int event);
 void gov_queue_work(struct dbs_data *dbs_data, struct cpufreq_policy *policy,
 		unsigned int delay, bool all_cpus);
+void gov_cancel_work(struct dbs_data *dbs_data,
+		struct cpufreq_policy *policy);
 void od_register_powersave_bias_handler(unsigned int (*f)
 		(struct cpufreq_policy *, unsigned int, unsigned int),
 		unsigned int powersave_bias);
diff --git a/drivers/cpufreq/cpufreq_hotplug.c b/drivers/cpufreq/cpufreq_hotplug.c
index 2715895e49b8..966f58138dfc 100755
--- a/drivers/cpufreq/cpufreq_hotplug.c
+++ b/drivers/cpufreq/cpufreq_hotplug.c
@@ -34,6 +34,7 @@
 #include <mach/io.h>
 #include <mach/register.h>
 #include <linux/sched/rt.h>
+#include <linux/notifier.h>
 #include "cpufreq_governor.h"
 
 /* greater than 80% avg load across online CPUs increases frequency */
@@ -66,6 +67,7 @@ static cpumask_var_t new_mask;
 struct cpufreq_governor cpufreq_gov_hotplug;
 #endif
 static struct task_struct *cpu_hotplug_task;
+static struct task_struct *cpu_idle_task;
 static int cpu_hotplug_flag = 0;
 static DEFINE_PER_CPU(struct hg_cpu_dbs_info_s, hp_cpu_dbs_info);
 
@@ -477,6 +479,37 @@ static void hg_exit(struct dbs_data *dbs_data)
 {
 	kfree(dbs_data->tuners);
 }
+static void cpu_idle_thread()
+{
+	int cpu = get_cpu();
+	put_cpu();
+	struct hg_cpu_dbs_info_s *dbs_info = &per_cpu(hg_cpu_dbs_info, cpu);
+	struct cpufreq_policy *policy = dbs_info->cdbs.cur_policy;
+	struct dbs_data *dbs_data = policy->governor_data;
+	struct cpu_dbs_common_info *cdbs = dbs_data->cdata->get_cpu_cdbs(cpu);
+	struct hg_dbs_tuners *hg_tuners = dbs_data->tuners;
+	unsigned int sampling_rate = hg_tuners->sampling_rate;
+
+	dbs_info = &per_cpu(hg_cpu_dbs_info, policy->cpu);
+	while(1){
+		if (kthread_should_stop())
+			break;
+		if(!mutex_trylock(&dbs_info->cdbs.timer_mutex))
+			goto wait_next_event;
+		if (!dbs_info->enable) {
+			mutex_unlock(&dbs_info->cdbs.timer_mutex);
+			goto wait_next_event;
+		}
+		gov_cancel_work(dbs_data, policy);
+		gov_queue_work(dbs_data, policy,
+						delay_for_sampling_rate(sampling_rate), false);
+		mutex_unlock(&dbs_info->cdbs.timer_mutex);
+wait_next_event:
+		set_current_state(TASK_INTERRUPTIBLE);
+		schedule();
+		set_current_state(TASK_RUNNING);
+	}
+}
 static void cpu_hotplug_thread(int *hotplug_flag)
 {
 	int i, j,target_cpu = 1;
@@ -605,9 +638,11 @@ static void hg_check_cpu(int cpu, unsigned int max_load)
 	/* check for frequency increase based on max_load */
 	if (max_load > hg_tuners->up_threshold) {
 		/* increase to highest frequency supported */
-		if (policy->cur < policy->max)
+		if (policy->cur < policy->max){
+			dbs_info->requested_freq = policy->max;
 			__cpufreq_driver_target(policy, policy->max,
 					CPUFREQ_RELATION_H);
+		}
 		goto out;
 	}
 
@@ -625,9 +660,8 @@ static void hg_check_cpu(int cpu, unsigned int max_load)
 		}
 	}
 
-	if ((max_load_freq >
-	    (hg_tuners->up_threshold - hg_tuners->down_differential) *
-	     policy->cur) && (policy->cur < policy->max)) {
+	if ((max_load > hg_tuners->up_threshold - hg_tuners->down_differential)
+		 &&(policy->cur < policy->max)) {
 		unsigned int freq_next;
 
 		freq_next = hispeed_freq;
@@ -640,7 +674,9 @@ static void hg_check_cpu(int cpu, unsigned int max_load)
 		if(freq_next == policy->cur)
 			goto out;
 
-		 __cpufreq_driver_target(policy, freq_next,
+		dbs_info->requested_freq = freq_next;
+
+		__cpufreq_driver_target(policy, freq_next,
 					 CPUFREQ_RELATION_L);
 		goto out;
 	}
@@ -663,7 +699,9 @@ static void hg_check_cpu(int cpu, unsigned int max_load)
 		if(freq_next == policy->cur)
 			goto out;
 
-		 __cpufreq_driver_target(policy, freq_next,
+		dbs_info->requested_freq = freq_next;
+
+		__cpufreq_driver_target(policy, freq_next,
 					 CPUFREQ_RELATION_L);
 	}
 out:
@@ -690,9 +728,72 @@ static void hg_dbs_timer(struct work_struct *work)
 	mutex_unlock(&dbs_info->cdbs.timer_mutex);
 }
 
+static void cpufreq_hotplug_idle_start(void)
+{
+#if 0
+	int cpu = get_cpu();
+	put_cpu();
+	struct hg_cpu_dbs_info_s *dbs_info = &per_cpu(hg_cpu_dbs_info, cpu);
+	struct cpufreq_policy *policy = dbs_info->cdbs.cur_policy;
+	struct dbs_data *dbs_data = policy->governor_data;
+	struct cpu_dbs_common_info *cdbs = dbs_data->cdata->get_cpu_cdbs(cpu);
+	struct hg_dbs_tuners *hg_tuners = dbs_data->tuners;
+	unsigned int sampling_rate = hg_tuners->sampling_rate;
+
+	dbs_info = &per_cpu(hg_cpu_dbs_info, policy->cpu);
+
+	if(!mutex_trylock(&dbs_info->cdbs.timer_mutex))
+		return;
+	if (!dbs_info->enable) {
+		mutex_unlock(&dbs_info->cdbs.timer_mutex);
+		return;
+	}
+
+	if (dbs_info->requested_freq != policy->min) {
+		/*
+		 * Entering idle while not at lowest speed.  On some
+		 * platforms this can hold the other CPU(s) at that speed
+		 * even though the CPU is idle. Set a timer to re-evaluate
+		 * speed so this idle CPU doesn't hold the other CPUs above
+		 * min indefinitely.  This should probably be a quirk of
+		 * the CPUFreq driver.
+		 */
+		if (!pending)
+			cpufreq_interactive_timer_resched(pcpu);
+	}
+
+	mutex_unlock(&dbs_info->cdbs.timer_mutex);
+#endif
+}
+static void cpufreq_hotplug_idle_end(void)
+{
+	wake_up_process(cpu_idle_task);
+}
+
+static int cpufreq_hotplug_idle_notifier(struct notifier_block *nb,
+					     unsigned long val, void *data)
+{
+	switch (val) {
+	case IDLE_START:
+		cpufreq_hotplug_idle_start();
+		break;
+	case IDLE_END:
+		cpufreq_hotplug_idle_end();
+		break;
+	}
+
+	return 0;
+}
+
+static struct notifier_block cpufreq_hotplug_idle_nb = {
+	.notifier_call = cpufreq_hotplug_idle_notifier,
+};
+
+
 define_get_cpu_dbs_routines(hg_cpu_dbs_info);
 
 static struct hg_ops hg_ops = {
+	.notifier_block = &cpufreq_hotplug_idle_nb,
 };
 
 static struct common_dbs_data hg_dbs_cdata = {
@@ -766,6 +867,17 @@ static int __init cpufreq_gov_dbs_init(void)
 	get_task_struct(cpu_hotplug_task);
 	cpu_hotplug_flag = CPU_HOTPLUG_NONE;
 
+	/*******************cpu idle task******************/
+	cpu_idle_task =
+		kthread_create_on_cpu(cpu_idle_thread, NULL, 0,
+			       "cpu_idle_gdbs");
+	if (IS_ERR(cpu_idle_task)){
+		printk("------ Error: create hotplug scaling idle thread fail\n");
+		return PTR_ERR(cpu_idle_task);
+	}
+	sched_setscheduler_nocheck(cpu_idle_task, SCHED_FIFO, &param);
+	get_task_struct(cpu_idle_task);
+
 	err = cpufreq_register_governor(&cpufreq_gov_hotplug);
 
 	return err;
@@ -775,6 +887,8 @@ static void __exit cpufreq_gov_dbs_exit(void)
 {
 	cpufreq_unregister_governor(&cpufreq_gov_hotplug);
 	free_cpumask_var(new_mask);
+	kthread_stop(cpu_idle_task);
+	put_task_struct(cpu_idle_task);
 	kthread_stop(NULL_task);
 	kthread_stop(cpu_hotplug_task);
 	put_task_struct(cpu_hotplug_task);
-- 
2.19.0

