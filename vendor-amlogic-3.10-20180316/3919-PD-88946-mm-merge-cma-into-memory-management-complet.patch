From cf5aaf203bca42c2f717a61e7302fc71d60ef233 Mon Sep 17 00:00:00 2001
From: "shi.liu" <shi.liu@amlogic.com>
Date: Wed, 16 Apr 2014 16:48:52 +0800
Subject: [PATCH 3919/5965] PD #88946: mm:  merge cma into memory management
 completed

---
 drivers/gpu/ion/ion_system_heap.c |  4 ++--
 include/linux/mmzone.h            |  1 +
 mm/page_alloc.c                   | 29 +++++++++++++++++++++++++----
 3 files changed, 28 insertions(+), 6 deletions(-)

diff --git a/drivers/gpu/ion/ion_system_heap.c b/drivers/gpu/ion/ion_system_heap.c
index af79bdf8f1a3..24f909daf76f 100644
--- a/drivers/gpu/ion/ion_system_heap.c
+++ b/drivers/gpu/ion/ion_system_heap.c
@@ -124,7 +124,7 @@ static struct page_info *alloc_largest_available(struct ion_system_heap *heap,
 	struct zone *zone = NULL;
 	struct zoneref *z;
 	struct zonelist *zonelist;
-	bool ret;
+	bool ret = true;
 
 	zonelist = NODE_DATA(numa_node_id())->node_zonelists;
 
@@ -132,7 +132,7 @@ static struct page_info *alloc_largest_available(struct ion_system_heap *heap,
 
 		for_each_zone_zonelist(zone, z, zonelist,
 				gfp_zone(heap->pools[order_to_index(orders[i])]->gfp_mask)) {
-			ret = zone_watermark_ok_safe(zone, orders[i], low_wmark_pages(zone), 0, 0);
+			ret = zone_watermark_ok_safe(zone, orders[i], high_wmark_pages(zone), 0, 0);
 			if(ret)
 				break;
 		}
diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 1431651e625e..f01ab52de0e6 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -84,6 +84,7 @@ static inline int get_pageblock_migratetype(struct page *page)
 struct free_area {
 	struct list_head	free_list[MIGRATE_TYPES];
 	unsigned long		nr_free;
+	unsigned long		nr_free_cma;
 };
 
 struct pglist_data;
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 2bcf9b237d11..ef4f0e577e60 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -583,6 +583,8 @@ static inline void __free_one_page(struct page *page,
 			list_del(&buddy->lru);
 			zone->free_area[order].nr_free--;
 			rmv_page_order(buddy);
+			if(is_migrate_cma(migratetype))
+				zone->free_area[order].nr_free_cma--;
 		}
 		combined_idx = buddy_idx & page_idx;
 		page = page + (combined_idx - page_idx);
@@ -615,6 +617,8 @@ static inline void __free_one_page(struct page *page,
 	list_add(&page->lru, &zone->free_area[order].free_list[migratetype]);
 out:
 	zone->free_area[order].nr_free++;
+	if(is_migrate_cma(migratetype))
+		zone->free_area[order].nr_free_cma++;
 }
 
 static inline int free_pages_check(struct page *page)
@@ -847,6 +851,8 @@ static inline void expand(struct zone *zone, struct page *page,
 #endif
 		list_add(&page[size].lru, &area->free_list[migratetype]);
 		area->nr_free++;
+		if(is_migrate_cma(migratetype))
+			area->nr_free_cma++;
 		set_page_order(&page[size], high);
 	}
 }
@@ -915,6 +921,8 @@ struct page *__rmqueue_smallest(struct zone *zone, unsigned int order,
 		list_del(&page->lru);
 		rmv_page_order(page);
 		area->nr_free--;
+		if(is_migrate_cma(migratetype))
+			area[order].nr_free_cma--;
 		expand(zone, page, order, current_order, area, migratetype);
 		return page;
 	}
@@ -1049,6 +1057,8 @@ __rmqueue_fallback(struct zone *zone, int order, int start_migratetype)
 			page = list_entry(area->free_list[migratetype].next,
 					struct page, lru);
 			area->nr_free--;
+			if(is_migrate_cma(migratetype))
+				area[order].nr_free_cma--;
 
 			/*
 			 * If breaking a large block of pages, move all free
@@ -1438,7 +1448,8 @@ static int __isolate_free_page(struct page *page, unsigned int order)
 	list_del(&page->lru);
 	zone->free_area[order].nr_free--;
 	rmv_page_order(page);
-
+	if(is_migrate_cma(mt))
+		zone->free_area[order].nr_free_cma--;
 	/* Set the pageblock if the isolated page is at least a pageblock */
 	if (order >= pageblock_order - 1) {
 		struct page *endpage = page + (1 << order) - 1;
@@ -1649,24 +1660,32 @@ static bool __zone_watermark_ok(struct zone *z, int order, unsigned long mark,
 		min -= min / 2;
 	if (alloc_flags & ALLOC_HARDER)
 		min -= min / 4;
+
 #ifdef CONFIG_CMA
 	/* If allocation can't use CMA areas don't use free CMA pages */
 	if (!(alloc_flags & ALLOC_CMA))
 		free_cma = zone_page_state(z, NR_FREE_CMA_PAGES);
 #endif
 
-	if (free_pages - free_cma <= min + lowmem_reserve)
+	free_pages -= free_cma;
+	if (free_pages <= min + lowmem_reserve)
 		return false;
+
 	for (o = 0; o < order; o++) {
 		/* At the next order, this order's pages become unavailable */
 		free_pages -= z->free_area[o].nr_free << o;
-
+#ifdef CONFIG_CMA
+	/* If allocation can't use CMA areas don't use free CMA pages */
+	if (!(alloc_flags & ALLOC_CMA))
+		free_pages += z->free_area[o].nr_free_cma << o;
+#endif
 		/* Require fewer higher order pages to be free */
 		min >>= min_free_order_shift;
 
 		if (free_pages <= min)
 			return false;
 	}
+
 	return true;
 }
 
@@ -2700,7 +2719,6 @@ out:
 		goto retry_cpuset;
 
 	memcg_kmem_commit_charge(page, memcg, order);
-
 	return page;
 }
 EXPORT_SYMBOL(__alloc_pages_nodemask);
@@ -3999,6 +4017,7 @@ static void __meminit zone_init_free_lists(struct zone *zone)
 	for_each_migratetype_order(order, t) {
 		INIT_LIST_HEAD(&zone->free_area[order].free_list[t]);
 		zone->free_area[order].nr_free = 0;
+		zone->free_area[order].nr_free_cma = 0;
 	}
 }
 
@@ -6167,6 +6186,8 @@ __offline_isolated_pages(unsigned long start_pfn, unsigned long end_pfn)
 		list_del(&page->lru);
 		rmv_page_order(page);
 		zone->free_area[order].nr_free--;
+		//if(is_migrate_cma(migratetype))
+		//	zone->free_area[order].nr_free_cma++;
 #ifdef CONFIG_HIGHMEM
 		if (PageHighMem(page))
 			totalhigh_pages -= 1 << order;
-- 
2.19.0

