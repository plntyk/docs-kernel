From 44dff8d1ba55a7e16c106e730ffd22a6f674bf1e Mon Sep 17 00:00:00 2001
From: "shi.liu" <shi.liu@amlogic.com>
Date: Tue, 4 Nov 2014 14:16:10 +0800
Subject: [PATCH 5055/5965] pd#99063:ashmem  deadlock

Change-Id: Ia7e2b657a689d1b9f6abc629201a7beb6f02c08f

pd#99063:mm: optimize for 512M s805 platform

Change-Id: I36eb338d8df930f0e644db45a0457b1148cb7e14

pd#99063:mm: optimize for 512M s805 platform

Change-Id: I7202dbb45dcbabf79c3816e4cb8ef1cfde9c9073
---
 drivers/staging/android/ashmem.c | 13 +++++-
 drivers/staging/zram/zram_drv.c  | 15 +++++--
 include/linux/mm_inline.h        |  1 +
 include/linux/mmzone.h           |  1 -
 mm/internal.h                    |  2 +
 mm/migrate.c                     |  4 +-
 mm/page-writeback.c              |  3 +-
 mm/page_alloc.c                  |  5 +--
 mm/vmscan.c                      | 76 ++++++++++++++++----------------
 mm/vmstat.c                      |  4 +-
 10 files changed, 73 insertions(+), 51 deletions(-)
 mode change 100644 => 100755 drivers/staging/zram/zram_drv.c

diff --git a/drivers/staging/android/ashmem.c b/drivers/staging/android/ashmem.c
index 3511b0840362..27f3b03233fb 100644
--- a/drivers/staging/android/ashmem.c
+++ b/drivers/staging/android/ashmem.c
@@ -356,6 +356,7 @@ out:
 static int ashmem_shrink(struct shrinker *s, struct shrink_control *sc)
 {
 	struct ashmem_range *range, *next;
+	struct task_struct *owner = NULL;
 
 	/* We might recurse into filesystem code, so bail out if necessary */
 	if (sc->nr_to_scan && !(sc->gfp_mask & __GFP_FS))
@@ -363,7 +364,12 @@ static int ashmem_shrink(struct shrinker *s, struct shrink_control *sc)
 	if (!sc->nr_to_scan)
 		return lru_count;
 
-	mutex_lock(&ashmem_mutex);
+#if defined(CONFIG_SMP)
+	owner = ashmem_mutex.owner;
+#endif
+	if(!(owner && (owner == current)))
+		mutex_lock(&ashmem_mutex);
+
 	list_for_each_entry_safe(range, next, &ashmem_lru_list, lru) {
 		loff_t start = range->pgstart * PAGE_SIZE;
 		loff_t end = (range->pgend + 1) * PAGE_SIZE;
@@ -371,6 +377,8 @@ static int ashmem_shrink(struct shrinker *s, struct shrink_control *sc)
 		do_fallocate(range->asma->file,
 				FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,
 				start, end - start);
+		if (!range_on_lru(range))
+			break;
 		range->purged = ASHMEM_WAS_PURGED;
 		lru_del(range);
 
@@ -378,7 +386,8 @@ static int ashmem_shrink(struct shrinker *s, struct shrink_control *sc)
 		if (sc->nr_to_scan <= 0)
 			break;
 	}
-	mutex_unlock(&ashmem_mutex);
+	if(!(owner && (owner == current)))
+		mutex_unlock(&ashmem_mutex);
 
 	return lru_count;
 }
diff --git a/drivers/staging/zram/zram_drv.c b/drivers/staging/zram/zram_drv.c
old mode 100644
new mode 100755
index 5baae0f94b6e..bef0cf9db034
--- a/drivers/staging/zram/zram_drv.c
+++ b/drivers/staging/zram/zram_drv.c
@@ -40,6 +40,7 @@ static int zram_major;
 struct zram *zram_devices;
 void *compress_addr = NULL;
 void *user_addr = NULL;
+void *meta_addr = NULL;
 /* Module params (documentation at end) */
 static unsigned int num_devices = 1;
 
@@ -605,7 +606,7 @@ static int zram_ioctl(struct block_device *bdev, fmode_t f_mode,
 
 	switch(f_mode){
 		case 80:
-		if(!compress_addr){
+		if(!compress_addr || !user_addr || !meta_addr){
 			*compress_len_temp = 0;
 			return -ENOMEM;
 		}
@@ -613,7 +614,7 @@ static int zram_ioctl(struct block_device *bdev, fmode_t f_mode,
 		uncmem = kmap_atomic((struct page *)page_addr);
 		user_mem = user_addr;
 		memcpy(user_mem, uncmem, PAGE_SIZE);
-		src = user_mem + PAGE_SIZE;
+		src = meta_addr;
 		compress_workmem = compress_addr;
 
 		ret = lzo1x_1_compress(user_mem, PAGE_SIZE, src, &clen,
@@ -738,11 +739,16 @@ static int __init zram_init(void)
 		ret = -ENOMEM;
 		goto out;
 	}
-	user_addr = kzalloc(PAGE_SIZE << 1, GFP_KERNEL);
+	user_addr = kzalloc(PAGE_SIZE, GFP_KERNEL);
 	if(!user_addr){
 		ret = -ENOMEM;
 		goto out;
 	}
+	meta_addr = kzalloc(PAGE_SIZE << 1, GFP_KERNEL);
+	if(!meta_addr){
+		ret = -ENOMEM;
+		goto out;
+	}
 
 	zram_major = register_blkdev(0, "zram");
 	if (zram_major <= 0) {
@@ -795,6 +801,9 @@ static void __exit zram_exit(void)
 	unregister_blkdev(zram_major, "zram");
 
 	kfree(zram_devices);
+	kfree(meta_addr);
+	kfree(compress_addr);
+	kfree(user_addr);
 	pr_debug("Cleanup done!\n");
 }
 
diff --git a/include/linux/mm_inline.h b/include/linux/mm_inline.h
index 1397ccf81e91..cf55945c83fb 100644
--- a/include/linux/mm_inline.h
+++ b/include/linux/mm_inline.h
@@ -2,6 +2,7 @@
 #define LINUX_MM_INLINE_H
 
 #include <linux/huge_mm.h>
+#include <linux/swap.h>
 
 /**
  * page_is_file_cache - should the page be on a file LRU or anon LRU?
diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index f01ab52de0e6..78ebc049333a 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -354,7 +354,6 @@ struct zone {
 	 * free areas of different sizes
 	 */
 	spinlock_t		lock;
-	int                     all_unreclaimable; /* All pages pinned */
 #if defined CONFIG_COMPACTION || defined CONFIG_CMA
 	/* Set to true when the PG_migrate_skip bits should be cleared */
 	bool			compact_blockskip_flush;
diff --git a/mm/internal.h b/mm/internal.h
index 8562de0a5197..45f7c449ef7d 100644
--- a/mm/internal.h
+++ b/mm/internal.h
@@ -90,6 +90,8 @@ extern unsigned long highest_memmap_pfn;
  */
 extern int isolate_lru_page(struct page *page);
 extern void putback_lru_page(struct page *page);
+extern unsigned long zone_reclaimable_pages(struct zone *zone);
+extern bool zone_reclaimable(struct zone *zone);
 
 /*
  * in mm/rmap.c:
diff --git a/mm/migrate.c b/mm/migrate.c
index a88c12f2235d..a357f359b5e7 100644
--- a/mm/migrate.c
+++ b/mm/migrate.c
@@ -1468,7 +1468,7 @@ static bool migrate_balanced_pgdat(struct pglist_data *pgdat,
 		if (!populated_zone(zone))
 			continue;
 
-		if (zone->all_unreclaimable)
+		if (!zone_reclaimable(zone))
 			continue;
 
 		/* Avoid waking kswapd by allocating pages_to_migrate pages. */
@@ -1549,7 +1549,7 @@ bool numamigrate_update_ratelimit(pg_data_t *pgdat, unsigned long nr_pages)
 	else
 		pgdat->numabalancing_migrate_nr_pages += nr_pages;
 	spin_unlock(&pgdat->numabalancing_migrate_lock);
-	
+
 	return rate_limited;
 }
 
diff --git a/mm/page-writeback.c b/mm/page-writeback.c
index 73cbc5dc150b..f7bdeefbfbcb 100644
--- a/mm/page-writeback.c
+++ b/mm/page-writeback.c
@@ -35,9 +35,10 @@
 #include <linux/buffer_head.h> /* __set_page_dirty_buffers */
 #include <linux/pagevec.h>
 #include <linux/timer.h>
+#include <linux/mm_inline.h>
 #include <linux/sched/rt.h>
 #include <trace/events/writeback.h>
-
+#include "internal.h"
 /*
  * Sleep at most 200ms at a time in balance_dirty_pages().
  */
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index b14297a58414..8b877b2ad7d8 100755
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -61,6 +61,7 @@
 #include <linux/hugetlb.h>
 #include <linux/sched/rt.h>
 #include <linux/sysctl.h>
+#include <linux/mm_inline.h>
 
 #include <asm/tlbflush.h>
 #include <asm/div64.h>
@@ -656,7 +657,6 @@ static void free_pcppages_bulk(struct zone *zone, int count,
 	int to_free = count;
 
 	spin_lock(&zone->lock);
-	zone->all_unreclaimable = 0;
 	zone->pages_scanned = 0;
 
 	while (to_free) {
@@ -705,7 +705,6 @@ static void free_one_page(struct zone *zone, struct page *page, int order,
 				int migratetype)
 {
 	spin_lock(&zone->lock);
-	zone->all_unreclaimable = 0;
 	zone->pages_scanned = 0;
 
 	__free_one_page(page, zone, order, migratetype);
@@ -3133,7 +3132,7 @@ void show_free_areas(unsigned int filter)
 			K(zone_page_state(zone, NR_FREE_CMA_PAGES)),
 			K(zone_page_state(zone, NR_WRITEBACK_TEMP)),
 			zone->pages_scanned,
-			(zone->all_unreclaimable ? "yes" : "no")
+			(!zone_reclaimable(zone) ? "yes" : "no")
 			);
 		printk("lowmem_reserve[]:");
 		for (i = 0; i < MAX_NR_ZONES; i++)
diff --git a/mm/vmscan.c b/mm/vmscan.c
index 7020a8e83d16..f57d483d3d67 100755
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -148,6 +148,25 @@ static bool global_reclaim(struct scan_control *sc)
 }
 #endif
 
+unsigned long zone_reclaimable_pages(struct zone *zone)
+{
+	int nr;
+
+	nr = zone_page_state(zone, NR_ACTIVE_FILE) +
+	     zone_page_state(zone, NR_INACTIVE_FILE);
+
+	if (get_nr_swap_pages() > 0)
+		nr += zone_page_state(zone, NR_ACTIVE_ANON) +
+		      zone_page_state(zone, NR_INACTIVE_ANON);
+
+	return nr;
+}
+
+bool zone_reclaimable(struct zone *zone)
+{
+	return zone->pages_scanned < zone_reclaimable_pages(zone) * 6;
+}
+
 static unsigned long get_lru_size(struct lruvec *lruvec, enum lru_list lru)
 {
 	if (!mem_cgroup_disabled())
@@ -1730,7 +1749,7 @@ static void get_scan_count(struct lruvec *lruvec, struct scan_control *sc,
 	 * latencies, so it's better to scan a minimum amount there as
 	 * well.
 	 */
-	if (current_is_kswapd() && zone->all_unreclaimable)
+	if (current_is_kswapd() && !zone_reclaimable(zone))
 		force_scan = true;
 	if (!global_reclaim(sc))
 		force_scan = true;
@@ -1786,7 +1805,7 @@ static void get_scan_count(struct lruvec *lruvec, struct scan_control *sc,
 	 * There is enough inactive page cache, do not reclaim
 	 * anything from the anonymous working set right now.
 	 */
-#if 0
+#if 1
 	if (!inactive_file_is_low(lruvec)) {
 		scan_balance = SCAN_FILE;
 		goto out;
@@ -2135,8 +2154,7 @@ static bool shrink_zones(struct zonelist *zonelist, struct scan_control *sc)
 		if (global_reclaim(sc)) {
 			if (!cpuset_zone_allowed_hardwall(zone, GFP_KERNEL))
 				continue;
-			if (zone->all_unreclaimable &&
-					sc->priority != DEF_PRIORITY)
+			if (sc->priority != DEF_PRIORITY && !zone_reclaimable(zone))
 				continue;	/* Let kswapd poll it */
 			if (IS_ENABLED(CONFIG_COMPACTION)) {
 				/*
@@ -2174,24 +2192,6 @@ static bool shrink_zones(struct zonelist *zonelist, struct scan_control *sc)
 	return aborted_reclaim;
 }
 
-static unsigned long zone_reclaimable_pages(struct zone *zone)
-{
-	int nr;
-
-	nr = zone_page_state(zone, NR_ACTIVE_FILE) +
-	     zone_page_state(zone, NR_INACTIVE_FILE);
-
-	if (get_nr_swap_pages() > 0)
-		nr += zone_page_state(zone, NR_ACTIVE_ANON) +
-		      zone_page_state(zone, NR_INACTIVE_ANON);
-
-	return nr;
-}
-
-static bool zone_reclaimable(struct zone *zone)
-{
-	return zone->pages_scanned < zone_reclaimable_pages(zone) * 6;
-}
 
 /* All zones in zonelist are unreclaimable? */
 static bool all_unreclaimable(struct zonelist *zonelist,
@@ -2206,7 +2206,7 @@ static bool all_unreclaimable(struct zonelist *zonelist,
 			continue;
 		if (!cpuset_zone_allowed_hardwall(zone, GFP_KERNEL))
 			continue;
-		if (!zone->all_unreclaimable)
+		if (zone_reclaimable(zone))
 			return false;
 	}
 
@@ -2626,7 +2626,7 @@ static bool pgdat_balanced(pg_data_t *pgdat, int order, int classzone_idx)
 		 * DEF_PRIORITY. Effectively, it considers them balanced so
 		 * they must be considered balanced here as well!
 		 */
-		if (zone->all_unreclaimable) {
+		if (!zone_reclaimable(zone)) {
 			balanced_pages += zone->managed_pages;
 			continue;
 		}
@@ -2737,8 +2737,8 @@ loop_again:
 			if (!populated_zone(zone))
 				continue;
 
-			if (zone->all_unreclaimable &&
-			    sc.priority != DEF_PRIORITY)
+			if (sc.priority != DEF_PRIORITY &&
+			    !zone_reclaimable(zone))
 				continue;
 
 			/*
@@ -2766,7 +2766,8 @@ loop_again:
 				zone_clear_flag(zone, ZONE_CONGESTED);
 			}
 		}
-
+		printk(KERN_DEBUG"%s end zone=%d, pgdat_is_balanced=%d\n", __func__,
+			   end_zone, pgdat_is_balanced);
 		if (i < 0) {
 			pgdat_is_balanced = true;
 			goto out;
@@ -2789,14 +2790,13 @@ loop_again:
 		 */
 		for (i = 0; i <= end_zone; i++) {
 			struct zone *zone = pgdat->node_zones + i;
-			int nr_slab, testorder;
+			int testorder;
 			unsigned long balance_gap;
 
 			if (!populated_zone(zone))
 				continue;
 
-			if (zone->all_unreclaimable &&
-			    sc.priority != DEF_PRIORITY)
+			if (sc.priority != DEF_PRIORITY&&!zone_reclaimable(zone))
 				continue;
 
 			sc.nr_scanned = 0;
@@ -2840,11 +2840,8 @@ loop_again:
 				shrink_zone(zone, &sc);
 
 				reclaim_state->reclaimed_slab = 0;
-				nr_slab = shrink_slab(&shrink, sc.nr_scanned, lru_pages);
+				shrink_slab(&shrink, sc.nr_scanned, lru_pages);
 				sc.nr_reclaimed += reclaim_state->reclaimed_slab;
-
-				if (nr_slab == 0 && !zone_reclaimable(zone))
-					zone->all_unreclaimable = 1;
 			}
 
 			/*
@@ -2854,7 +2851,7 @@ loop_again:
 			if (sc.priority < DEF_PRIORITY - 2)
 				sc.may_writepage = 1;
 
-			if (zone->all_unreclaimable) {
+			if (!zone_reclaimable(zone)) {
 				if (end_zone && end_zone == i)
 					end_zone--;
 				continue;
@@ -2870,7 +2867,7 @@ loop_again:
 				 */
 				zone_clear_flag(zone, ZONE_CONGESTED);
 		}
-
+		printk(KERN_DEBUG"%s, nr_reclaimed=%lu\n",__func__, sc.nr_reclaimed);
 		/*
 		 * If the low watermark is met there is no need for processes
 		 * to be throttled on pfmemalloc_wait as they should not be
@@ -3383,6 +3380,8 @@ static int __zone_reclaim(struct zone *zone, gfp_t gfp_mask, unsigned int order)
 	}
 
 	nr_slab_pages0 = zone_page_state(zone, NR_SLAB_RECLAIMABLE);
+	printk(KERN_DEBUG"%s, nr_slab_pags0=%d, min_slab_pages=%d\n", __func__, nr_slab_pages0,
+		   zone->min_slab_pages);
 	if (nr_slab_pages0 > zone->min_slab_pages) {
 		/*
 		 * shrink_slab() does not currently allow us to determine how
@@ -3396,7 +3395,7 @@ static int __zone_reclaim(struct zone *zone, gfp_t gfp_mask, unsigned int order)
 		 */
 		for (;;) {
 			unsigned long lru_pages = zone_reclaimable_pages(zone);
-
+			printk(KERN_DEBUG"%s,	lru_pages=%d\n", __func__, lru_pages);
 			/* No reclaimable slab or very low memory pressure */
 			if (!shrink_slab(&shrink, sc.nr_scanned, lru_pages))
 				break;
@@ -3413,6 +3412,7 @@ static int __zone_reclaim(struct zone *zone, gfp_t gfp_mask, unsigned int order)
 		 * reclaimed from this zone.
 		 */
 		nr_slab_pages1 = zone_page_state(zone, NR_SLAB_RECLAIMABLE);
+		printk(KERN_DEBUG"%s, nr_slab_pags0=%d\n", __func__, nr_slab_pages0);
 		if (nr_slab_pages1 < nr_slab_pages0)
 			sc.nr_reclaimed += nr_slab_pages0 - nr_slab_pages1;
 	}
@@ -3442,7 +3442,7 @@ int zone_reclaim(struct zone *zone, gfp_t gfp_mask, unsigned int order)
 	    zone_page_state(zone, NR_SLAB_RECLAIMABLE) <= zone->min_slab_pages)
 		return ZONE_RECLAIM_FULL;
 
-	if (zone->all_unreclaimable)
+	if (!zone_reclaimable(zone))
 		return ZONE_RECLAIM_FULL;
 
 	/*
diff --git a/mm/vmstat.c b/mm/vmstat.c
index 66ed1d0ff7c3..c8c51d66c68e 100755
--- a/mm/vmstat.c
+++ b/mm/vmstat.c
@@ -19,6 +19,8 @@
 #include <linux/math64.h>
 #include <linux/writeback.h>
 #include <linux/compaction.h>
+#include <linux/mm_inline.h>
+#include "internal.h"
 
 #ifdef CONFIG_VM_EVENT_COUNTERS
 DEFINE_PER_CPU(struct vm_event_state, vm_event_states) = {{0}};
@@ -1054,7 +1056,7 @@ static void zoneinfo_show_print(struct seq_file *m, pg_data_t *pgdat,
 		   "\n  all_unreclaimable: %u"
 		   "\n  start_pfn:         %lu"
 		   "\n  inactive_ratio:    %u",
-		   zone->all_unreclaimable,
+		   !zone_reclaimable(zone),
 		   zone->zone_start_pfn,
 		   zone->inactive_ratio);
 	seq_putc(m, '\n');
-- 
2.19.0

