From f0b08a39002941f4968ef63f874835cd4e561e2f Mon Sep 17 00:00:00 2001
From: "shi.liu" <shi.liu@amlogic.com>
Date: Wed, 14 Jan 2015 10:35:17 +0800
Subject: [PATCH 5344/5965] pd#100612(99160):cma: optimize for low mem platform
 and cma allocation

Change-Id: I5a6a109413850138de6b1f88bd223733a8b80428

pd#100612(101972-1):mm: oom concurrence bug

Change-Id: Ib06954a16d233664d5f295905d019da216f27ed9
---
 mm/oom_kill.c   | 13 ++++++++++++-
 mm/page_alloc.c | 36 +++++++++++++++++++-----------------
 mm/vmscan.c     |  2 +-
 3 files changed, 32 insertions(+), 19 deletions(-)
 mode change 100644 => 100755 mm/oom_kill.c

diff --git a/mm/oom_kill.c b/mm/oom_kill.c
old mode 100644
new mode 100755
index dfa94ed3c7fa..bc5bca7f127b
--- a/mm/oom_kill.c
+++ b/mm/oom_kill.c
@@ -1,6 +1,6 @@
 /*
  *  linux/mm/oom_kill.c
- * 
+ *
  *  Copyright (C)  1998,2000  Rik van Riel
  *	Thanks go out to Claus Fischer for some serious inspiration and
  *	for goading me into coding this file...
@@ -438,6 +438,12 @@ void oom_kill_process(struct task_struct *p, gfp_t gfp_mask, int order,
 	 */
 	read_lock(&tasklist_lock);
 	do {
+		if (p->flags & PF_EXITING){
+			pr_err("%s:  process %d (%s) has been exiting\n",
+				message, task_pid_nr(p), p->comm);
+			break;
+		}
+
 		list_for_each_entry(child, &t->children, sibling) {
 			unsigned int child_points;
 
@@ -457,6 +463,11 @@ void oom_kill_process(struct task_struct *p, gfp_t gfp_mask, int order,
 		}
 	} while_each_thread(p, t);
 	read_unlock(&tasklist_lock);
+	if (p->flags & PF_EXITING) {
+		set_tsk_thread_flag(p, TIF_MEMDIE);
+		put_task_struct(p);
+		return;
+	}
 
 	rcu_read_lock();
 	p = find_lock_task_mm(victim);
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 8178cab72328..f712ce60aebd 100755
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -1038,7 +1038,9 @@ __rmqueue_fallback(struct zone *zone, int order, int start_migratetype)
 	int current_order;
 	struct page *page;
 	int migratetype, i;
+	int flags = start_migratetype & __GFP_BDEV;
 
+	start_migratetype &= (~__GFP_BDEV);
 	/* Find the largest possible block of pages in the other list */
 	for (current_order = MAX_ORDER-1; current_order >= order;
 						--current_order) {
@@ -1048,7 +1050,8 @@ __rmqueue_fallback(struct zone *zone, int order, int start_migratetype)
 			/* MIGRATE_RESERVE handled later if necessary */
 			if (migratetype == MIGRATE_RESERVE)
 				break;
-
+			if(flags && migratetype == MIGRATE_CMA)
+				continue;
 			area = &(zone->free_area[current_order]);
 			if (list_empty(&area->free_list[migratetype]))
 				continue;
@@ -1121,6 +1124,7 @@ static struct page *__rmqueue(struct zone *zone, unsigned int order,
 {
 	struct page *page;
 #ifdef CONFIG_CMA
+	int ori_migratetype = migratetype;
 	int i = 0;
 	int tmp_migratetype = MIGRATE_RESERVE;
 	int flags = migratetype & __GFP_BDEV;
@@ -1128,16 +1132,14 @@ static struct page *__rmqueue(struct zone *zone, unsigned int order,
 
 #ifdef CONFIG_CMA
 	if(flags){
-		migratetype &= ~__GFP_BDEV;
+		ori_migratetype &= ~__GFP_BDEV;
 	}
-	if(migratetype == MIGRATE_MOVABLE){
+	if(ori_migratetype == MIGRATE_MOVABLE){
 		for (i = 0;; i++) {
-			tmp_migratetype = fallbacks[migratetype][i];
+			tmp_migratetype = fallbacks[ori_migratetype][i];
 			if (tmp_migratetype == MIGRATE_CMA){
-				if(flags){
-					fallbacks[migratetype][i] = MIGRATE_RESERVE;
-					goto retry_reserve;
-				}
+				if(flags)
+					tmp_migratetype = MIGRATE_RESERVE;
 				break;
 			}
 			if (tmp_migratetype == MIGRATE_RESERVE)
@@ -1145,16 +1147,17 @@ static struct page *__rmqueue(struct zone *zone, unsigned int order,
 		}
 		if (tmp_migratetype == MIGRATE_CMA){
 			page = __rmqueue_smallest(zone, order, MIGRATE_CMA);
-			if(page)
+			if(page){
+				ori_migratetype = MIGRATE_CMA;
 				goto alloc_page_success;
+			}
 		}
-
 	}
 #endif
 retry_reserve:
-	page = __rmqueue_smallest(zone, order, migratetype);
+	page = __rmqueue_smallest(zone, order, ori_migratetype);
 
-	if (unlikely(!page) && migratetype != MIGRATE_RESERVE) {
+	if (unlikely(!page) && ori_migratetype != MIGRATE_RESERVE) {
 		page = __rmqueue_fallback(zone, order, migratetype);
 
 		/*
@@ -1163,17 +1166,14 @@ retry_reserve:
 		 * and we want just one call site
 		 */
 		if (!page) {
-			migratetype = MIGRATE_RESERVE;
+			ori_migratetype = MIGRATE_RESERVE;
 			goto retry_reserve;
 		}
 	}
 #ifdef CONFIG_CMA
 alloc_page_success:
-	if(flags && (migratetype == MIGRATE_MOVABLE) && (tmp_migratetype == MIGRATE_CMA)){
-		fallbacks[migratetype][i] = MIGRATE_CMA;
-	}
 #endif
-	trace_mm_page_alloc_zone_locked(page, order, migratetype);
+	trace_mm_page_alloc_zone_locked(page, order, ori_migratetype);
 	return page;
 }
 
@@ -1585,6 +1585,8 @@ again:
 				spin_unlock(&zone->lock);
 				if (!page)
 					goto failed;
+				__mod_zone_freepage_state(zone, -(1 << order),
+							  get_pageblock_migratetype(page));
 				goto alloc_sucess;
 			}
 		}
diff --git a/mm/vmscan.c b/mm/vmscan.c
index 100821fa2130..70fa9ed8254c 100755
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -1818,7 +1818,7 @@ static void get_scan_count(struct lruvec *lruvec, struct scan_control *sc,
 	 * This scanning priority is essentially the inverse of IO cost.
 	 */
 	anon_prio = vmscan_swappiness(sc);
-	if(vm_swap_full()){
+	if(atomic_long_read(&nr_swap_pages) * 3 < total_swap_pages){
 		anon_prio >>= 1;
 	}
 	file_prio = 200 - anon_prio;
-- 
2.19.0

